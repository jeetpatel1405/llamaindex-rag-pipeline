# ğŸ¦™ LlamaIndex-Powered Vector Similarity Search Engine for Document Retrieval

ğŸ“– **[Documentation](https://docs.llamaindex.ai/en/stable/)** | ğŸ’» **[Source Code](https://github.com/jeetpatel1405/llamaindex-rag-pipeline)**

A high-performance, LLM-ready RAG pipeline that enables semantic document retrieval using LlamaIndex, SentenceTransformers, and HNSWlib. Build your own scalable AI search system for documents with blazing-fast retrieval, semantic understanding, and real-world applications.

---



## ğŸ—ï¸ System Architecture

![LlamaIndex RAG Pipeline](./Users/jeetpatel/Downloads/LLamaIndex/demo/llamaindex-rag-pipeline.pngllamaindex-rag-pipeline.png)

---
## ğŸŒğŸ” Project Overview



<details> 
<summary><strong>1ï¸âƒ£ Data Ingestion from Any Source</strong></summary>

ğŸ“¥ We support multiple unstructured/structured data formats:

ğŸ“„ PDFâ€ƒğŸŒ HTMLâ€ƒğŸ“Š CSVâ€ƒğŸ“ DOCXâ€ƒâ• More...

ğŸ”Œ These are handled via Data Connectors, powered by the [LlamaHub](https://llamahub.ai) open-source community, which converts them into page by page Document Objects each associated with its parent file.


#### ğŸ§ª Example Code

```python
from llama_index.readers import SimpleDirectoryReader

documents = SimpleDirectoryReader(input_dir="./data").load_data()
```


</details>

<details> 
<summary><strong>2ï¸âƒ£ Document Parsing & Embedding</strong></summary>

ğŸ“š Each document goes through the following steps:

ğŸ”¨ Chunked into smaller "Nodes"  
ğŸ”¢ Embedded into vector space using your configured embedding model (e.g., OpenAI, HuggingFace)

ğŸ“Œ Flow:
```
Document --> Chunking --> Node

```

#### ğŸ§ª Example: Document Parsing & Embedding Configuration

```python
from llama_index.node_parser import SentenceWindowNodeParser
from llama_index import Settings

def prepare_documents_for_embedding(documents, llm, embed_model="local:BAAI/bge-small-en-v1.5"):
    # Configure node parser for sentence window style
    node_parser = SentenceWindowNodeParser.from_defaults(
        window_size=3,
        window_metadata_key="window",
        original_text_metadata_key="original_text",
    )

    # Set global settings
    Settings.node_parser = node_parser
    Settings.llm = llm
    Settings.embed_model = embed_model

    return documents  # You may return processed nodes if needed in other configs

# Usage
documents = prepare_documents_for_embedding(documents, llm=my_llm)
```

</details>

<details> 
<summary><strong>3ï¸âƒ£ Indexing</strong></summary>

ğŸ—ƒï¸ Nodes are stored in an Index, enabling fast and intelligent retrieval. LlamaIndex supports multiple index types, each with a distinct purpose:

ğŸ“Œ Flow:
```
Node Parser --> Embedding --> VectorDB

```

- ğŸ”¹ **VectorStoreIndex** â€“ The default index for semantic search using vector similarity.  
  *Use case:* Asking questions over discrete document chunks (e.g., PDFs, knowledge bases).

- ğŸ”¹ **SlidingWindowIndex** â€“ Maintains cross-chunk context using overlapping windows.  
  *Use case:* Ideal for long, flowing text like books or transcripts.

- ğŸ”¹ **AutoMergeIndex** â€“ Builds a hierarchical summary tree by merging nodes recursively.  
  *Use case:* Summarizing large document sets (e.g., logs, meeting notes).

âš™ï¸ You can plug in vector databases like **FAISS**, **Chroma**, **Weaviate**, or **Pinecone** when using `VectorStoreIndex` for scalable and efficient semantic search.

ğŸ’¾ Additionally, **persistent storage** is supported through the `StorageContext` class. This allows you to **save and reload indexes** across sessions using local disk paths:


âš™ï¸ You can plug in vector databases like **FAISS**, **Chroma**, **Weaviate**, or **Pinecone** when using `VectorStoreIndex` for scalable and efficient semantic search.

ğŸ’¾ Additionally, **persistent storage** is supported through the `StorageContext` class. This allows you to **save and reload indexes** across sessions using local disk paths:

#### ğŸ§ª Example: Saving and Loading Persistent Indexes

```python
from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage

# Build and persist index
index = VectorStoreIndex.from_documents(documents)
index.storage_context.persist(persist_dir="./my_index")

# Later: Load the same index
storage_context = StorageContext.from_defaults(persist_dir="./my_index")
index = load_index_from_storage(storage_context)
```

This makes it easy to avoid rebuilding embeddings every time, and enables quick startup for production or iterative workflows.


</details>

<details> 
<summary><strong>4ï¸âƒ£ Query Processing & Retrieval</strong></summary>

ğŸ§‘ User asks a natural language query â†’  
ğŸ§­ The Router decides which retriever(s) to use â†’  
ğŸ” Retriever(s) pull the most relevant nodes from the index.

This enables:

- Hybrid retrieval (semantic + keyword)
- Multiple index fusion
- Modular design with multiple retrievers

#### ğŸ§ª Example: Query Engine with Sentence Window Reranking

```python
from llama_index.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank

def get_sentence_window_query_engine(
    sentence_index,
    similarity_top_k=6,
    rerank_top_n=2,
):
    # Define postprocessors
    postproc = MetadataReplacementPostProcessor(target_metadata_key="window")
    rerank = SentenceTransformerRerank(
        top_n=rerank_top_n, model="BAAI/bge-reranker-base"
    )

    # Create a query engine with similarity search + reranking
    sentence_window_engine = sentence_index.as_query_engine(
        similarity_top_k=similarity_top_k,
        node_postprocessors=[postproc, rerank]
    )
    
    return sentence_window_engine
  
# Usage
query_engine = get_sentence_window_query_engine(sentence_index=index)
```


</details>

<details> 
<summary><strong>5ï¸âƒ£ Response Generation</strong></summary>

ğŸ§  Retrieved chunks are passed to the Response Synthesizer, which uses an LLM (like GPT-4, Claude, or Llama) to generate a grounded, well-formed response.

ğŸ“Œ Flow:
```
Query + Retrieved Context --> LLM --> Final Answer âœ¨
```

#### ğŸ§ª Example Code

```python
from llama_index.llms import OpenAI
from llama_index.response_synthesizers import CompactAndRefine

llm = OpenAI(model="gpt-4")
query_engine.response_synthesizer = CompactAndRefine(llm=llm)

final_response = query_engine.query("Give me a summary of key financial insights.")
print(final_response)
```

</details>

---

## ğŸ“Š Performance Metrics

| Metric              | Value        | Description                                      |
|---------------------|--------------|--------------------------------------------------|
| Query Time          | < 1 sec      | With local vector DB and reranker                |
| Vector Dimensions   | 768D         | SentenceTransformer embeddings                   |
| Index Persistence   | Yes          | Saved via `StorageContext`                       |
| Search Accuracy     | ~95%         | Based on semantic similarity test sets           |
| Scale               | 50k+ docs    | Performance tested on large corpora              |

---

## ğŸŒ Tech Stack

- ğŸ” **Vector Search**: HNSWlib for approximate nearest neighbor (ANN) search
- ğŸ§  **Embeddings**: SentenceTransformers (BAAI/bge-small-en-v1.5 & reranker-base)
- ğŸ¦™ **RAG Framework**: LlamaIndex with Sentence Window/Automerge Indexing
- âš™ï¸ **Index Persistence**: LlamaIndex `StorageContext` for local disk-based saving/loading
- ğŸ§± **Query Engine**: Metadata-based postprocessing and reranking
- ğŸŒ **API Framework**: FastAPI (for RESTful endpoint integration)
- ğŸ³ **Deployment Ready**: Docker-compatible and lightweight for Kubernetes


---

## ğŸš€ Quick Start

### ğŸ”§ Prerequisites

- Python 3.9+
- pip
- (Optional) Docker, StorageContext, or GPU/Google Colab for acceleration

---

### ğŸ“‚ Project Setup

1. **Add your documents** to the `./data` folder (PDFs, DOCX, TXT, etc.)

2. **Download required models** and place them in the `./models` directory:
   - ğŸ”— [openchat_3.5.Q4_K_M.gguf](https://huggingface.co/TheBloke/openchat_3.5-GGUF/blob/main/openchat_3.5.Q4_K_M.gguf) â€” a quantized GGUF LLM format for local inference

   > You can use this model with LLM-compatible frameworks like `llama-cpp` or integrate it via custom LLM wrappers supported by LlamaIndex.

---

### âš™ï¸ Index Creation

Run the following command to generate your vector index:

```bash
python createindex.py
```

This script processes the documents, creates sentence window embeddings, and saves the index in the `sentence_index` directory.

---

### â–¶ï¸ Run the API

Once the index is built, launch the Simple Gradio service:

```bash
cd api
python3 api.py
```

Access the interface at: [http://localhost:7860](http://localhost:7860)

---

## ğŸ› ï¸ Development Roadmap

âœ… Completed
- Sentence window parsing and Automerge index parsing
- LLM synthesis with GPT4/
- Persistent vector index with FAISS
- Modular query engine with postprocessors

ğŸš§ In Progress
- Web demo with interactive UI via OpenWebUI
- API layer using FastAPI



---


## ğŸ‘¨â€ğŸ’» Author

Built by **Jeet Patel**  
ğŸŒ [GitHub](https://github.com/jeetpatel1405) | [LinkedIn](https://www.linkedin.com/in/jeet1405/)

---

## ğŸ”— Links

- ğŸ“‚ [Source Code](https://github.com/jeetpatel1405/llamaindex-rag-pipeline)
- ğŸ“– [Documentation](https://docs.llamaindex.ai/en/stable/)
- ğŸ› [Report Issues](https://github.com/jeetpatel1405/llamaindex-rag-pipeline/issues)
